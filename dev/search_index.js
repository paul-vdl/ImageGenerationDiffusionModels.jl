var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here's a quick example to get started:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"In Pkg-mode:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"add https://github.com/paul-vdl/ImageGenerationDiffusionModels.jl","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Then in REPL:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using ImageGenerationDiffusionModels","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"img = generate_grid()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"noisy_img = apply_noise(img)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"train_brain()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"denoised_img = denoise_image(img[1:32, 1:32])","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"new_img = generate_image_from_noise()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"! The last two functions create a png with the same name so you can only see one at the time","category":"page"},{"location":"#ImageGenerationDiffusionModels","page":"Home","title":"ImageGenerationDiffusionModels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ImageGenerationDiffusionModels.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#ImageGenerationDiffusionModels.apply_noise-Tuple{Any}","page":"Home","title":"ImageGenerationDiffusionModels.apply_noise","text":"apply_noise(img; num_noise_steps = 500, beta_min = 0.0001, beta_max = 0.02)\n\nApplies forward-noise to an image This function adds Gaussian noise to an image during multiple steps, which corresponds to the forward process in diffusion models.\n\nArguments\n\nimg : The input image\nnum_noise_steps: number of steps over which noise should be added to the image (500 by default).\nbeta_min: Minimum beta value (0.0001 by default)\nbeta_max: Maximum beta value (0.02 by default)\n\nReturns\n\nAn image with noise\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.build_unet","page":"Home","title":"ImageGenerationDiffusionModels.build_unet","text":"build_unet(in_ch::Int=1, out_ch::Int=1, time_dim::Int=256)\n\nBuilds a time-conditioned U-Net model for image denoising and generation in diffusion models\n\nArguments\n\nin_ch::Int=1: Number of unput channels(1 is for grayscale)\nout_ch::Int=1: Number of output channels(1 is for grayscale)\ntime_dim::Int=256: Dimensionality of the time embedding vector used for condition\n\nReturns\n\nA callable function (x, t_vec) -> output, where:\nx: Input image\nt_vec: time step vector\noutput: tensor of same dimensions as out_ch channels\n\n\n\n\n\n","category":"function"},{"location":"#ImageGenerationDiffusionModels.denoise_image-Tuple{AbstractMatrix{<:Real}}","page":"Home","title":"ImageGenerationDiffusionModels.denoise_image","text":"denoise_image(noisy_img)\n\nDenoises a noisy image using the trained neural network 'model'. Given a single input noisy_img::Matrix{<:Real}, this function produces a denoised version of that input file\n\nArguments\n\nnoisy_img::Matrix{<:Real}: noisy image\n\nReturns\n\nA denoised version of the image\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.down_block-Tuple{Any, Any, Any}","page":"Home","title":"ImageGenerationDiffusionModels.down_block","text":"down_block(in_ch, out_ch, time_dim)\n\nCreates a downsampling block for the U-Net\n\nArguments\n\nin_ch::Int: Number of input channels\nout_ch::Int: Number of output channels\ntime_dim::Int: Dimensionality of the time embedding vector used for conditioning\n\nReturns\n\nA callable function (x, t_emb) -> (down, skip), where:\nx: Input feature map\nt_emb: Time embedding vector for the current step\ndown: Downsampled feature map for the next layer\nskip: Intermediate feature map\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.generate_grid-Tuple{}","page":"Home","title":"ImageGenerationDiffusionModels.generate_grid","text":"generate_grid()\n\nLoads the digits data and generates grid\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.generate_image_from_noise-Tuple{}","page":"Home","title":"ImageGenerationDiffusionModels.generate_image_from_noise","text":"generate_image_from_noise()\n\nGenerates a new image from random noise and denoises it.\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.get_data-Tuple{Any}","page":"Home","title":"ImageGenerationDiffusionModels.get_data","text":"get_data(batch_size)\n\nHelper function that loads MNIST images and returns loader.\n\nArguments\n\nbatch_size::Int: size of batch\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.pad_or_crop-Tuple{Any, Any}","page":"Home","title":"ImageGenerationDiffusionModels.pad_or_crop","text":"pad_or_crop(x, ref)\n\nPads or crops the input tensor x so that its dimensions match those of ref\n\nArguments\n\nx: A 4D tensor, typically shaped (C, H, W, N)\nref: A reference tensor whose spatial size (H, W) x should match\n\nReturns\n\nA tensor with the same number of channels and batch size as x, but with height and width adjusted to match ref\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.sinusoidal_embedding-Tuple{Vector{Float32}, Int64}","page":"Home","title":"ImageGenerationDiffusionModels.sinusoidal_embedding","text":"sinusoidal_embedding(t::Vector{Float32}, dim::Int)\n\nGenerates sinusoidal positional embeddings from a vector of scalar inputs, typically used to encode time steps or sequence positions\n\nArguments\n\nt::Vector{Float32}: A vector of time or position values\ndim::Int: The desired embedding dimensionality\n\nReturns\n\nA matrix of shape (length(t), dim) where each row is the embedding of one time step\n\n\n\n\n\n","category":"method"},{"location":"#ImageGenerationDiffusionModels.up_block-Tuple{Any, Any, Any}","page":"Home","title":"ImageGenerationDiffusionModels.up_block","text":"up_block(in_ch, out_ch, time_dim)\n\nCreates an upsampling block used in U-Net\n\nArguments\n\nin_ch::Int: Number of input channels to the block\nout_ch::Int: Number of output channels after the convolutions\ntime_dim::Int: Dimensionality of the time embedding vector\n\nReturns\n\nA callable function (x, skip, t_emb) -> output, where:\nx: The upsampled feature map from the previous layer\nskip: The skip connection feature map from the encoder\nt_emb: The time embedding vector for the current step\nThe output is a feature map with out_ch channels\n\n\n\n\n\n","category":"method"}]
}
